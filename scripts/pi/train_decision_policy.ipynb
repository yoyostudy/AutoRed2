{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30698,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "DecisionMaker",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoyostudy/RL4LM_PI/blob/main/scripts/pi/train_decision_policy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TL,DR:\n",
        "\n",
        "- This file is for **training** and **inference**\n",
        "- High level policy to decide what action(Attack or Attempt) to take based on the current observation(llm_response)\n",
        "- fine tune model: DistilBertForSequenceClassification\n",
        "- base model: distilbert-base-uncased\n",
        "- Trainer: Supervised Fine tuning"
      ],
      "metadata": {
        "id": "oPuhxydTWjEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel, DistilBertForSequenceClassification\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import torch as th\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class FinetuneTagger:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_path: str = \"https://github.com/HumanCompatibleAI/tensor-trust-data/raw/main/detecting-extractions/v1/prompt_extraction_detection.jsonl\",\n",
        "        model_name: str = 'distilbert-base-uncased',\n",
        "        epoch: int = 3,\n",
        "        batch_size: int = 64,\n",
        "        lr: int = 5e-5,\n",
        "        device: str = 'cuda',\n",
        "        seed: int = 42,\n",
        "        train_ratio: float = 0.8,\n",
        "        val_ratio: float = 0.1):\n",
        "\n",
        "        self.seed = seed\n",
        "\n",
        "        self.epoch = epoch\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = DistilBertForSequenceClassification.from_pretrained(model_name,\n",
        "                                                                         num_labels=2,\n",
        "                                                                         problem_type=\"multi_label_classification\")\n",
        "        self.model.to(self.device)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr = lr, eps =1e-8)\n",
        "\n",
        "        self.load_data(\n",
        "            data_path,\n",
        "            train_ratio,\n",
        "            val_ratio\n",
        "        )\n",
        "\n",
        "    def load_data(self, data_path, train_ratio, val_ratio):\n",
        "        df = pd.read_json(data_path, lines = True).set_index('sample_id')\n",
        "        df = df.sample(frac=1, random_state=self.seed).reset_index(drop=True)\n",
        "\n",
        "        size = len(df)\n",
        "        train_data = df.iloc[: int(train_ratio*size)]\n",
        "        val_data = df.iloc[int(train_ratio*size): int(train_ratio*size+val_ratio*size)]\n",
        "        test_data = df.iloc[int(train_ratio*size+val_ratio*size):]\n",
        "\n",
        "        def create_data_loader(data):\n",
        "            labels = data['is_prompt_extraction'].to_numpy().astype(int)\n",
        "            one_hot_labels = th.eye(2)[labels].to(self.device)\n",
        "            obs = data['llm_output'].tolist()\n",
        "\n",
        "            encode_obs = self.tokenizer(obs,\n",
        "                                        truncation = True,\n",
        "                                        padding = 'max_length',\n",
        "                                        add_special_tokens = False,\n",
        "                                        max_length = 64,\n",
        "                                        return_tensors = 'pt').to(self.device)\n",
        "            encode_obs_list = [{key: encode_obs[key][i] for key in encode_obs} for i in range(len(encode_obs['input_ids']))]\n",
        "\n",
        "            paired_data = list(zip(encode_obs_list, one_hot_labels))\n",
        "            return DataLoader(paired_data, batch_size=self.batch_size)\n",
        "\n",
        "        self.train_loader = create_data_loader(train_data)\n",
        "        self.val_loader = create_data_loader(val_data)\n",
        "        self.test_loader = create_data_loader(test_data)\n",
        "\n",
        "    def train(self):\n",
        "        self.model.train()\n",
        "        for epoch in range(self.epoch):\n",
        "            with tqdm(self.train_loader) as train_iter:\n",
        "                for batch_obs, batch_label in train_iter:\n",
        "                    batch_input_ids = batch_obs['input_ids']\n",
        "                    batch_input_attn = batch_obs['attention_mask']\n",
        "\n",
        "                    loss = self.model(\n",
        "                        batch_input_ids,\n",
        "                        batch_input_attn,\n",
        "                        labels = batch_label\n",
        "                    ).loss\n",
        "\n",
        "                    self.optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    self.optimizer.step()\n",
        "                    train_iter.set_description(\"loss: %f\" % loss)\n",
        "\n",
        "    def test(self):\n",
        "        self.model.eval()\n",
        "        true_labels = []\n",
        "        predicted_labels = []\n",
        "\n",
        "        with tqdm(self.test_loader) as test_iter:\n",
        "            for batch_obs, batch_label in test_iter:\n",
        "                with th.no_grad():\n",
        "                    logits = self.model(**batch_obs).logits\n",
        "\n",
        "                predicted_label = logits.argmax(dim=1).detach().cpu().numpy().tolist()\n",
        "                predicted_labels.extend(predicted_label)\n",
        "\n",
        "                true_label = th.argmax(batch_label, dim=1).tolist()\n",
        "                true_labels.extend(true_label)\n",
        "\n",
        "        accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "        return accuracy"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T23:35:03.426916Z",
          "iopub.execute_input": "2024-05-04T23:35:03.42727Z",
          "iopub.status.idle": "2024-05-04T23:35:03.449081Z",
          "shell.execute_reply.started": "2024-05-04T23:35:03.427242Z",
          "shell.execute_reply": "2024-05-04T23:35:03.447947Z"
        },
        "trusted": true,
        "id": "vLnAS-_6V8G2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "INFERENCE = False\n",
        "\n",
        "seeds = [10, 1032, 18]\n",
        "device = 'cuda'\n",
        "acs = []\n",
        "\n",
        "def main(seed):\n",
        "    ckp_path = f'/content/drive/My Drive/RL4LM_PI/hlp_{seed}/'\n",
        "    trainer = FinetuneTagger(epoch = 20, seed = seed, device = device)\n",
        "\n",
        "    th.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if device == 'cuda':\n",
        "        th.cuda.manual_seed_all(seed)\n",
        "    th.backends.cudnn.deterministic = True\n",
        "    th.backends.cudnn.benchmark = False\n",
        "\n",
        "    if INFERENCE:\n",
        "        model = DistilBertForSequenceClassification.from_pretrained(ckp_path)\n",
        "        model.to(device)\n",
        "        trainer.model = model\n",
        "    else:\n",
        "        trainer.train()\n",
        "        trainer.model.save_pretrained(ckp_path)\n",
        "\n",
        "    ac = trainer.test()\n",
        "    print('\\naccuracy', ac)\n",
        "    return ac\n",
        "\n",
        "for seed in seeds:\n",
        "    ac = main(seed)\n",
        "    acs.append(ac)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T23:35:03.450687Z",
          "iopub.execute_input": "2024-05-04T23:35:03.450964Z",
          "iopub.status.idle": "2024-05-04T23:35:12.056445Z",
          "shell.execute_reply.started": "2024-05-04T23:35:03.450942Z",
          "shell.execute_reply": "2024-05-04T23:35:12.055465Z"
        },
        "trusted": true,
        "id": "Y_ydqT8XV8G5",
        "outputId": "8ba433a9-a068-4881-eb11-05a9b0d5dad6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "loss: 0.667863: 100%|██████████| 3/3 [00:00<00:00, 10.70it/s]\n",
            "loss: 0.602856: 100%|██████████| 3/3 [00:00<00:00, 13.03it/s]\n",
            "loss: 0.538267: 100%|██████████| 3/3 [00:00<00:00, 13.11it/s]\n",
            "loss: 0.482991: 100%|██████████| 3/3 [00:00<00:00, 13.07it/s]\n",
            "loss: 0.423442: 100%|██████████| 3/3 [00:00<00:00, 13.10it/s]\n",
            "loss: 0.359725: 100%|██████████| 3/3 [00:00<00:00, 13.09it/s]\n",
            "loss: 0.301161: 100%|██████████| 3/3 [00:00<00:00, 13.12it/s]\n",
            "loss: 0.256299: 100%|██████████| 3/3 [00:00<00:00, 13.13it/s]\n",
            "loss: 0.180711: 100%|██████████| 3/3 [00:00<00:00, 13.17it/s]\n",
            "loss: 0.186861: 100%|██████████| 3/3 [00:00<00:00, 13.13it/s]\n",
            "loss: 0.200751: 100%|██████████| 3/3 [00:00<00:00, 13.18it/s]\n",
            "loss: 0.159119: 100%|██████████| 3/3 [00:00<00:00, 13.19it/s]\n",
            "loss: 0.128141: 100%|██████████| 3/3 [00:00<00:00, 13.15it/s]\n",
            "loss: 0.130709: 100%|██████████| 3/3 [00:00<00:00, 13.14it/s]\n",
            "loss: 0.129928: 100%|██████████| 3/3 [00:00<00:00, 13.17it/s]\n",
            "loss: 0.133774: 100%|██████████| 3/3 [00:00<00:00, 13.18it/s]\n",
            "loss: 0.147344: 100%|██████████| 3/3 [00:00<00:00, 13.13it/s]\n",
            "loss: 0.131135: 100%|██████████| 3/3 [00:00<00:00, 13.19it/s]\n",
            "loss: 0.119939: 100%|██████████| 3/3 [00:00<00:00, 13.16it/s]\n",
            "loss: 0.118477: 100%|██████████| 3/3 [00:00<00:00, 13.14it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 79.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "accuracy 0.782608695652174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "loss: 0.655816: 100%|██████████| 3/3 [00:00<00:00, 13.04it/s]\n",
            "loss: 0.592397: 100%|██████████| 3/3 [00:00<00:00, 12.94it/s]\n",
            "loss: 0.525523: 100%|██████████| 3/3 [00:00<00:00, 13.09it/s]\n",
            "loss: 0.453057: 100%|██████████| 3/3 [00:00<00:00, 13.12it/s]\n",
            "loss: 0.387752: 100%|██████████| 3/3 [00:00<00:00, 13.12it/s]\n",
            "loss: 0.371096: 100%|██████████| 3/3 [00:00<00:00, 13.11it/s]\n",
            "loss: 0.291171: 100%|██████████| 3/3 [00:00<00:00, 13.12it/s]\n",
            "loss: 0.261308: 100%|██████████| 3/3 [00:00<00:00, 13.13it/s]\n",
            "loss: 0.254730: 100%|██████████| 3/3 [00:00<00:00, 13.05it/s]\n",
            "loss: 0.251886: 100%|██████████| 3/3 [00:00<00:00, 13.10it/s]\n",
            "loss: 0.244920: 100%|██████████| 3/3 [00:00<00:00, 13.12it/s]\n",
            "loss: 0.201443: 100%|██████████| 3/3 [00:00<00:00, 13.03it/s]\n",
            "loss: 0.194748: 100%|██████████| 3/3 [00:00<00:00, 13.12it/s]\n",
            "loss: 0.173617: 100%|██████████| 3/3 [00:00<00:00, 13.15it/s]\n",
            "loss: 0.167985: 100%|██████████| 3/3 [00:00<00:00, 13.02it/s]\n",
            "loss: 0.188484: 100%|██████████| 3/3 [00:00<00:00, 13.12it/s]\n",
            "loss: 0.183557: 100%|██████████| 3/3 [00:00<00:00, 13.06it/s]\n",
            "loss: 0.197912: 100%|██████████| 3/3 [00:00<00:00, 13.18it/s]\n",
            "loss: 0.210662: 100%|██████████| 3/3 [00:00<00:00, 13.02it/s]\n",
            "loss: 0.193366: 100%|██████████| 3/3 [00:00<00:00, 13.01it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 74.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "accuracy 0.6086956521739131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "loss: 0.654940: 100%|██████████| 3/3 [00:00<00:00, 12.96it/s]\n",
            "loss: 0.579123: 100%|██████████| 3/3 [00:00<00:00, 12.96it/s]\n",
            "loss: 0.502756: 100%|██████████| 3/3 [00:00<00:00, 12.95it/s]\n",
            "loss: 0.451908: 100%|██████████| 3/3 [00:00<00:00, 12.98it/s]\n",
            "loss: 0.380036: 100%|██████████| 3/3 [00:00<00:00, 12.91it/s]\n",
            "loss: 0.393174: 100%|██████████| 3/3 [00:00<00:00, 12.95it/s]\n",
            "loss: 0.300638: 100%|██████████| 3/3 [00:00<00:00, 13.10it/s]\n",
            "loss: 0.300372: 100%|██████████| 3/3 [00:00<00:00, 13.07it/s]\n",
            "loss: 0.240181: 100%|██████████| 3/3 [00:00<00:00, 13.10it/s]\n",
            "loss: 0.212090: 100%|██████████| 3/3 [00:00<00:00, 13.15it/s]\n",
            "loss: 0.237906: 100%|██████████| 3/3 [00:00<00:00, 13.07it/s]\n",
            "loss: 0.228663: 100%|██████████| 3/3 [00:00<00:00, 12.98it/s]\n",
            "loss: 0.236605: 100%|██████████| 3/3 [00:00<00:00, 12.93it/s]\n",
            "loss: 0.226091: 100%|██████████| 3/3 [00:00<00:00, 12.89it/s]\n",
            "loss: 0.208850: 100%|██████████| 3/3 [00:00<00:00, 13.15it/s]\n",
            "loss: 0.218182: 100%|██████████| 3/3 [00:00<00:00, 13.11it/s]\n",
            "loss: 0.201380: 100%|██████████| 3/3 [00:00<00:00, 13.13it/s]\n",
            "loss: 0.198271: 100%|██████████| 3/3 [00:00<00:00, 13.11it/s]\n",
            "loss: 0.216702: 100%|██████████| 3/3 [00:00<00:00, 13.09it/s]\n",
            "loss: 0.204983: 100%|██████████| 3/3 [00:00<00:00, 13.10it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 80.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "accuracy 0.6086956521739131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "seed_ac_map = zip(seeds, acs)\n",
        "mean_accuracy = np.mean(acs)\n",
        "print(mean_accuracy)\n",
        "variance_accuracy = np.var(acs)\n",
        "print(variance_accuracy)"
      ],
      "metadata": {
        "id": "1w3szfxfzPBx",
        "outputId": "e7cf4079-e9e3-4d8e-d3c7-2484ebe8f241",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6666666666666666\n",
            "0.006721277042638102\n"
          ]
        }
      ]
    }
  ]
}