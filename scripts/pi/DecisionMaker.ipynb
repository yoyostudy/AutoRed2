{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30698,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "DecisionMaker",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoyostudy/RL4LM_PI/blob/main/scripts/pi/DecisionMaker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TL,DR:\n",
        "\n",
        "- High level policy to decide what action(Attack or Attempt) to take based on the current observation(llm_response)\n",
        "- fine tune model: DistilBertForSequenceClassification\n",
        "- base model: distilbert-base-uncased\n",
        "- Trainer: Supervised Fine tuning"
      ],
      "metadata": {
        "id": "oPuhxydTWjEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel, DistilBertForSequenceClassification\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import torch as th\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class FinetuneTagger:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_path: str = \"https://github.com/HumanCompatibleAI/tensor-trust-data/raw/main/detecting-extractions/v1/prompt_extraction_detection.jsonl\",\n",
        "        model_name: str = 'distilbert-base-uncased',\n",
        "        epoch: int = 3,\n",
        "        batch_size: int = 64,\n",
        "        lr: int = 5e-5,\n",
        "        device: str = 'cuda',\n",
        "        seed: int = 42,\n",
        "        train_ratio: float = 0.8,\n",
        "        val_ratio: float = 0.1):\n",
        "\n",
        "        self.seed = seed\n",
        "        self.epoch = epoch\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = DistilBertForSequenceClassification.from_pretrained(model_name,\n",
        "                                                                         num_labels=2,\n",
        "                                                                         problem_type=\"multi_label_classification\")\n",
        "        self.model.to(self.device)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr = lr, eps =1e-8)\n",
        "\n",
        "        self.load_data(\n",
        "            data_path,\n",
        "            train_ratio,\n",
        "            val_ratio\n",
        "        )\n",
        "\n",
        "    def load_data(self, data_path, train_ratio, val_ratio):\n",
        "        df = pd.read_json(data_path, lines = True).set_index('sample_id')\n",
        "        df = df.sample(frac=1, random_state=self.seed).reset_index(drop=True)\n",
        "\n",
        "        size = len(df)\n",
        "        train_data = df.iloc[: int(train_ratio*size)]\n",
        "        val_data = df.iloc[int(train_ratio*size): int(train_ratio*size+val_ratio*size)]\n",
        "        test_data = df.iloc[int(train_ratio*size+val_ratio*size):]\n",
        "\n",
        "        def create_data_loader(data):\n",
        "            labels = data['is_prompt_extraction'].to_numpy().astype(int)\n",
        "            one_hot_labels = th.eye(2)[labels].to(self.device)\n",
        "            obs = data['llm_output'].tolist()\n",
        "\n",
        "            encode_obs = self.tokenizer(obs,\n",
        "                                        truncation = True,\n",
        "                                        padding = 'max_length',\n",
        "                                        add_special_tokens = False,\n",
        "                                        max_length = 64,\n",
        "                                        return_tensors = 'pt').to(self.device)\n",
        "            encode_obs_list = [{key: encode_obs[key][i] for key in encode_obs} for i in range(len(encode_obs['input_ids']))]\n",
        "\n",
        "            paired_data = list(zip(encode_obs_list, one_hot_labels))\n",
        "            return DataLoader(paired_data, batch_size=self.batch_size)\n",
        "\n",
        "        self.train_loader = create_data_loader(train_data)\n",
        "        self.val_loader = create_data_loader(val_data)\n",
        "        self.test_loader = create_data_loader(test_data)\n",
        "\n",
        "    def train(self):\n",
        "        self.model.train()\n",
        "        for epoch in range(self.epoch):\n",
        "            with tqdm(self.train_loader) as train_iter:\n",
        "                for batch_obs, batch_label in train_iter:\n",
        "                    batch_input_ids = batch_obs['input_ids']\n",
        "                    batch_input_attn = batch_obs['attention_mask']\n",
        "\n",
        "                    loss = self.model(\n",
        "                        batch_input_ids,\n",
        "                        batch_input_attn,\n",
        "                        labels = batch_label\n",
        "                    ).loss\n",
        "\n",
        "                    self.optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    self.optimizer.step()\n",
        "                    train_iter.set_description(\"loss: %f\" % loss)\n",
        "\n",
        "    def test(self):\n",
        "        self.model.eval()\n",
        "        true_labels = []\n",
        "        predicted_labels = []\n",
        "\n",
        "        with tqdm(self.test_loader) as test_iter:\n",
        "            for batch_obs, batch_label in test_iter:\n",
        "                with th.no_grad():\n",
        "                    logits = self.model(**batch_obs).logits\n",
        "\n",
        "                predicted_label = logits.argmax(dim=1).detach().cpu().numpy().tolist()\n",
        "                predicted_labels.extend(predicted_label)\n",
        "\n",
        "                true_label = th.argmax(batch_label, dim=1).tolist()\n",
        "                true_labels.extend(true_label)\n",
        "\n",
        "        accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "        return accuracy"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T23:35:03.426916Z",
          "iopub.execute_input": "2024-05-04T23:35:03.42727Z",
          "iopub.status.idle": "2024-05-04T23:35:03.449081Z",
          "shell.execute_reply.started": "2024-05-04T23:35:03.427242Z",
          "shell.execute_reply": "2024-05-04T23:35:03.447947Z"
        },
        "trusted": true,
        "id": "vLnAS-_6V8G2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = FinetuneTagger(epoch = 15, seed = 1032)\n",
        "trainer.train()\n",
        "ac = trainer.test()\n",
        "print('\\naccuracy', ac)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-04T23:35:03.450687Z",
          "iopub.execute_input": "2024-05-04T23:35:03.450964Z",
          "iopub.status.idle": "2024-05-04T23:35:12.056445Z",
          "shell.execute_reply.started": "2024-05-04T23:35:03.450942Z",
          "shell.execute_reply": "2024-05-04T23:35:12.055465Z"
        },
        "trusted": true,
        "id": "Y_ydqT8XV8G5",
        "outputId": "35a75741-69f9-4542-e16a-afa1a6b71f7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "loss: 0.651541: 100%|██████████| 3/3 [00:00<00:00, 11.23it/s]\n",
            "loss: 0.601324: 100%|██████████| 3/3 [00:00<00:00, 13.01it/s]\n",
            "loss: 0.533172: 100%|██████████| 3/3 [00:00<00:00, 13.09it/s]\n",
            "loss: 0.485060: 100%|██████████| 3/3 [00:00<00:00, 13.12it/s]\n",
            "loss: 0.420088: 100%|██████████| 3/3 [00:00<00:00, 13.10it/s]\n",
            "loss: 0.359957: 100%|██████████| 3/3 [00:00<00:00, 13.05it/s]\n",
            "loss: 0.295863: 100%|██████████| 3/3 [00:00<00:00, 12.92it/s]\n",
            "loss: 0.242902: 100%|██████████| 3/3 [00:00<00:00, 13.12it/s]\n",
            "loss: 0.219501: 100%|██████████| 3/3 [00:00<00:00, 13.12it/s]\n",
            "loss: 0.213745: 100%|██████████| 3/3 [00:00<00:00, 13.07it/s]\n",
            "loss: 0.194471: 100%|██████████| 3/3 [00:00<00:00, 13.03it/s]\n",
            "loss: 0.194350: 100%|██████████| 3/3 [00:00<00:00, 13.09it/s]\n",
            "loss: 0.211567: 100%|██████████| 3/3 [00:00<00:00, 12.91it/s]\n",
            "loss: 0.177526: 100%|██████████| 3/3 [00:00<00:00, 13.13it/s]\n",
            "loss: 0.149927: 100%|██████████| 3/3 [00:00<00:00, 13.00it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 81.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "accuracy 0.6956521739130435\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}